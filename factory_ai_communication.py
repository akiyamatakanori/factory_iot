import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime, timedelta
import time
import json
from typing import Dict, List, Any
import uuid
import tiktoken  # ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ãƒˆç”¨
import requests  # Ollama APIç”¨

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="AI Multi-Agent Communication System",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        text-align: center;
        margin-bottom: 2rem;
    }
    .agent-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1.5rem;
        border-radius: 10px;
        color: white;
        margin-bottom: 1rem;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }
    .communication-box {
        background-color: #f8f9fa;
        border-left: 4px solid #667eea;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 5px;
        font-family: 'Courier New', monospace;
    }
    .agent-message {
        background-color: #e7f3ff;
        border-left: 4px solid #2196F3;
        padding: 0.8rem;
        margin: 0.5rem 0;
        border-radius: 5px;
    }
    .traffic-high {
        color: #dc3545;
        font-weight: bold;
    }
    .traffic-medium {
        color: #ffc107;
        font-weight: bold;
    }
    .traffic-low {
        color: #28a745;
        font-weight: bold;
    }
    .message-flow {
        display: flex;
        align-items: center;
        margin: 0.5rem 0;
        padding: 0.5rem;
        background-color: #fff;
        border-radius: 5px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
    }
</style>
""", unsafe_allow_html=True)

# ================================
# Ollama LLMçµ±åˆ
# ================================
OLLAMA_API_URL = "http://localhost:11434/api/generate"
OLLAMA_MODEL = "llama3.1:8b"

def call_ollama(prompt: str, system_prompt: str = "") -> str:
    """Ollama APIã‚’å‘¼ã³å‡ºã—ã¦LLMå¿œç­”ã‚’å–å¾—"""
    try:
        full_prompt = f"{system_prompt}\n\n{prompt}" if system_prompt else prompt
        
        response = requests.post(
            OLLAMA_API_URL,
            json={
                "model": OLLAMA_MODEL,
                "prompt": full_prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "max_tokens": 500
                }
            },
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            return result.get('response', '')
        else:
            return f"[Error: Ollama API returned status {response.status_code}]"
            
    except requests.exceptions.ConnectionError:
        return "[Error: Ollamaã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚`ollama serve`ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„]"
    except Exception as e:
        return f"[Error: {str(e)}]"

def check_ollama_status() -> Dict[str, Any]:
    """Ollamaã®çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            llama_installed = any(OLLAMA_MODEL in model.get('name', '') for model in models)
            return {
                'status': 'running',
                'models': models,
                'llama_installed': llama_installed
            }
    except:
        pass
    
    return {
        'status': 'not_running',
        'models': [],
        'llama_installed': False
    }

# ================================
# ãƒˆãƒ¼ã‚¯ãƒ³æ–™é‡‘è¨ˆç®—
# ================================
# 2024å¹´11æœˆæ™‚ç‚¹ã®æ–™é‡‘ï¼ˆUSD per 1M tokensï¼‰
TOKEN_PRICES = {
    'gpt-4o': {'input': 2.50, 'output': 10.00},
    'claude-sonnet-4': {'input': 3.00, 'output': 15.00},
    'gemini-1.5-pro': {'input': 1.25, 'output': 5.00}
}

# å›ºå®šç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆ
USD_TO_JPY = 155

def count_tokens(text: str, model: str = "gpt-4o") -> int:
    """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
    try:
        # GPT-4o/Claude/Geminiã®è¿‘ä¼¼ã¨ã—ã¦cl100k_baseã‚’ä½¿ç”¨
        encoding = tiktoken.get_encoding("cl100k_base")
        tokens = encoding.encode(str(text))
        return len(tokens)
    except Exception as e:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ–‡å­—æ•° / 4 ã§è¿‘ä¼¼
        return len(str(text)) // 4

def calculate_cost(tokens: int, model: str, message_type: str = 'input') -> float:
    """ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‹ã‚‰æ–™é‡‘ã‚’è¨ˆç®—ï¼ˆUSDï¼‰"""
    if model not in TOKEN_PRICES:
        return 0.0
    
    price_per_million = TOKEN_PRICES[model][message_type]
    return (tokens / 1_000_000) * price_per_million

# ================================
# AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåŸºåº•ã‚¯ãƒ©ã‚¹
# ================================
class AIAgent:
    def __init__(self, name: str, role: str):
        self.name = name
        self.role = role
        self.message_queue = []
        self.processing_log = []
        self.status = "idle"
        
    def send_message(self, to_agent: str, message_type: str, data: Dict, priority: str = "normal"):
        """ä»–ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡"""
        # dataã‚’JSON serializableå½¢å¼ã«å¤‰æ›
        serializable_data = self._make_serializable(data)
        
        # JSONæ–‡å­—åˆ—åŒ–
        json_str = json.dumps(serializable_data)
        
        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—
        token_count = count_tokens(json_str)
        
        message = {
            "id": str(uuid.uuid4()),
            "from": self.name,
            "to": to_agent,
            "type": message_type,
            "data": serializable_data,
            "priority": priority,
            "timestamp": datetime.now(),
            "size_kb": len(json_str) / 1024,  # é€šä¿¡é‡(KB)
            "tokens": token_count,  # ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            "cost_gpt4o": calculate_cost(token_count, 'gpt-4o', 'input'),
            "cost_claude_sonnet": calculate_cost(token_count, 'claude-sonnet-4', 'input'),
            "cost_gemini_pro": calculate_cost(token_count, 'gemini-1.5-pro', 'input')
        }
        return message
    
    def _make_serializable(self, obj):
        """ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’JSON serializableå½¢å¼ã«å¤‰æ›"""
        if isinstance(obj, dict):
            return {key: self._make_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, pd.DataFrame):
            # DataFrameã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›ï¼ˆåˆ—ã”ã¨ï¼‰
            result = {}
            for col in obj.columns:
                if pd.api.types.is_datetime64_any_dtype(obj[col]):
                    result[col] = obj[col].astype(str).tolist()
                else:
                    result[col] = obj[col].tolist()
            return result
        elif isinstance(obj, (pd.Timestamp, datetime)):
            return obj.isoformat()
        elif isinstance(obj, (np.integer, np.floating)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, pd.Series):
            return obj.tolist()
        else:
            return obj
    
    def receive_message(self, message: Dict):
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å—ä¿¡"""
        self.message_queue.append(message)
        
    def process(self, input_data: Any) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆå„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§å®Ÿè£…ï¼‰"""
        raise NotImplementedError
        
    def log_action(self, action: str, details: str):
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³è¨˜éŒ²"""
        self.processing_log.append({
            "timestamp": datetime.now(),
            "action": action,
            "details": details
        })

# ================================
# å„AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè£…
# ================================

class ProcessControlAgent(AIAgent):
    """ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    def __init__(self):
        super().__init__("ProcessControl", "ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã¨æœ€é©åŒ–")
        self.target_temperature = 75.0
        self.target_pressure = 1013.0
        
    def process(self, sensor_data: pd.DataFrame) -> Dict:
        """ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€æ¬¡ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«æŒ‡ç¤ºï¼ˆLLMçµ±åˆï¼‰"""
        self.status = "processing"
        self.log_action("process_analysis", "ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã®åˆ†æé–‹å§‹")
        
        # åˆ¶å¾¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨ˆç®—ï¼ˆ3ã¤ã®ãƒ©ã‚¤ãƒ³ã®å¹³å‡ï¼‰
        avg_temp = (sensor_data['temperature_line1'].mean() + 
                   sensor_data['temperature_line2'].mean() + 
                   sensor_data['temperature_line3'].mean()) / 3
        avg_pressure = (sensor_data['pressure_pump1'].mean() + 
                       sensor_data['pressure_pump2'].mean() + 
                       sensor_data['pressure_pump3'].mean()) / 3
        temp_deviation = abs(avg_temp - self.target_temperature)
        pressure_deviation = abs(avg_pressure - self.target_pressure)
        
        # ğŸ¤– LLMã§åˆ¶å¾¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–
        llm_prompt = f"""ã‚ãªãŸã¯å·¥å ´ã®ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚

ç¾åœ¨ã®çŠ¶æ³:
- å¹³å‡æ¸©åº¦: {avg_temp:.2f}Â°Cï¼ˆç›®æ¨™: {self.target_temperature}Â°Cã€åå·®: {temp_deviation:.2f}Â°Cï¼‰
- å¹³å‡åœ§åŠ›: {avg_pressure:.2f} hPaï¼ˆç›®æ¨™: {self.target_pressure} hPaã€åå·®: {pressure_deviation:.2f} hPaï¼‰
- Line 1æ¸©åº¦: {sensor_data['temperature_line1'].mean():.2f}Â°C
- Line 2æ¸©åº¦: {sensor_data['temperature_line2'].mean():.2f}Â°C
- Line 3æ¸©åº¦: {sensor_data['temperature_line3'].mean():.2f}Â°C

åˆ¶å¾¡ã™ã¹ãç‚¹ã‚’3ã¤ä»¥å†…ã§ç°¡æ½”ã«ææ¡ˆã—ã¦ãã ã•ã„ã€‚"""

        llm_response = call_ollama(
            llm_prompt,
            system_prompt="ã‚ãªãŸã¯å·¥å ´ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã®å°‚é–€å®¶ã§ã™ã€‚ç°¡æ½”ã«3ã¤ä»¥å†…ã§ææ¡ˆã—ã¦ãã ã•ã„ã€‚"
        )
        
        self.log_action("llm_analysis", f"LLMåˆ†æçµæœ: {llm_response[:100]}...")
        
        # ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ã®æŒ‡ç¤ºã‚’ç”Ÿæˆ
        message_to_collector = self.send_message(
            to_agent="DataCollection",
            message_type="request_detailed_data",
            data={
                "reason": "åˆ¶å¾¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è©³ç´°åˆ†æãŒå¿…è¦",
                "target_sensors": ["temperature_line1", "temperature_line2", "temperature_line3", 
                                 "pressure_pump1", "pressure_pump2", "pressure_pump3", 
                                 "vibration_motor1", "vibration_motor2", "vibration_motor3"],
                "sampling_rate": "high" if temp_deviation > 5 else "normal",
                "time_window": "last_10_minutes",
                "llm_recommendation": llm_response
            },
            priority="high" if temp_deviation > 10 else "normal"
        )
        
        # ç•°å¸¸æ¤œçŸ¥ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ã®é€šçŸ¥
        message_to_anomaly = self.send_message(
            to_agent="AnomalyDetection",
            message_type="control_status_update",
            data={
                "temperature_status": "warning" if temp_deviation > 5 else "normal",
                "pressure_status": "warning" if pressure_deviation > 10 else "normal",
                "control_actions_taken": ["temperature_adjustment"] if temp_deviation > 5 else [],
                "llm_analysis": llm_response
            },
            priority="medium"
        )
        
        self.status = "completed"
        return {
            "messages_sent": [message_to_collector, message_to_anomaly],
            "analysis": {
                "avg_temp": avg_temp,
                "avg_pressure": avg_pressure,
                "temp_deviation": temp_deviation,
                "pressure_deviation": pressure_deviation
            }
        }

class DataCollectionAgent(AIAgent):
    """ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    def __init__(self):
        super().__init__("DataCollection", "ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã®åé›†ã¨å‰å‡¦ç†")
        self.collection_rate = 10000  # pts/secï¼ˆ10å€ã«å¢—åŠ ï¼‰
        
    def process(self, request_message: Dict = None) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿åé›†ã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“é€šä¿¡ï¼ˆå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰"""
        self.status = "processing"
        self.log_action("data_collection", "å¤§è¦æ¨¡ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
        
        # ğŸ’¥ ãƒ‡ãƒ¼ã‚¿é‡ã‚’å¤§å¹…ã«å¢—åŠ ï¼š1,000ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ Ã— 20ã‚»ãƒ³ã‚µãƒ¼
        n_points = 1000  # 100 â†’ 1000ã«å¢—åŠ 
        n_sensors = 20    # 6 â†’ 20ã‚»ãƒ³ã‚µãƒ¼ã«å¢—åŠ 
        
        timestamps = pd.date_range(end=datetime.now(), periods=n_points, freq='10s')
        
        # åŸºæœ¬ã‚»ãƒ³ã‚µãƒ¼
        sensor_data = pd.DataFrame({
            'timestamp': timestamps,
            'temperature_line1': 75 + np.random.randn(n_points) * 3,
            'temperature_line2': 73 + np.random.randn(n_points) * 2.5,
            'temperature_line3': 76 + np.random.randn(n_points) * 3.2,
            'pressure_pump1': 1013 + np.random.randn(n_points) * 5,
            'pressure_pump2': 1010 + np.random.randn(n_points) * 4.8,
            'pressure_pump3': 1015 + np.random.randn(n_points) * 5.2,
            'vibration_motor1': 0.5 + np.abs(np.random.randn(n_points) * 0.1),
            'vibration_motor2': 0.48 + np.abs(np.random.randn(n_points) * 0.09),
            'vibration_motor3': 0.52 + np.abs(np.random.randn(n_points) * 0.11),
            'power_line1': 250 + np.random.randn(n_points) * 20,
            'power_line2': 245 + np.random.randn(n_points) * 18,
            'power_line3': 255 + np.random.randn(n_points) * 22,
            'production_rate_line1': 95 + np.random.randn(n_points) * 3,
            'production_rate_line2': 93 + np.random.randn(n_points) * 2.8,
            'production_rate_line3': 97 + np.random.randn(n_points) * 3.2,
            'humidity': 45 + np.random.randn(n_points) * 5,
            'air_quality': 80 + np.random.randn(n_points) * 10,
            'noise_level': 65 + np.random.randn(n_points) * 8,
            'flow_rate': 100 + np.random.randn(n_points) * 15,
            'rotation_speed': 1500 + np.random.randn(n_points) * 50
        })
        
        # ç•°å¸¸å€¤ã‚’è¿½åŠ ï¼ˆ5%ï¼‰
        anomaly_indices = np.random.choice(n_points, size=int(n_points * 0.05), replace=False)
        for idx in anomaly_indices:
            sensor_data.loc[idx, 'temperature_line1'] += np.random.choice([-15, 15])
            sensor_data.loc[idx, 'vibration_motor1'] += 0.5
        
        # ğŸ’¾ ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        try:
            save_dir = "/Users/takaakiy/factory_iot/SensorData"
            timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
            csv_filename = f"{save_dir}/sensor_data_{timestamp_str}.csv"
            sensor_data.to_csv(csv_filename, index=False)
            self.log_action("data_saved", f"ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {csv_filename}")
        except Exception as e:
            self.log_action("data_save_error", f"ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        message_to_process = self.send_message(
            to_agent="ProcessControl",
            message_type="data_ready",
            data={
                "data_summary": {
                    "points_collected": len(sensor_data),
                    "sensors": list(sensor_data.columns),
                    "quality": "high",
                    "collection_rate": self.collection_rate,
                    "total_datapoints": len(sensor_data) * len(sensor_data.columns),
                    "saved_to": csv_filename if 'csv_filename' in locals() else "N/A"
                }
            },
            priority="normal"
        )
        
        # ğŸ’¥ ç•°å¸¸æ¤œçŸ¥ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿è»¢é€
        message_to_anomaly = self.send_message(
            to_agent="AnomalyDetection",
            message_type="sensor_data_batch",
            data={
                "raw_data": {
                    col: sensor_data[col].tolist() for col in sensor_data.columns
                },
                "metadata": {
                    "collection_start": timestamps[0].isoformat(),
                    "collection_end": timestamps[-1].isoformat(),
                    "total_points": len(sensor_data),
                    "total_sensors": len(sensor_data.columns),
                    "sampling_interval": "10s",
                    "data_quality_score": 0.98
                }
            },
            priority="high"
        )
        
        # å“è³ªåˆ†æã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ã‚‚ãƒ‡ãƒ¼ã‚¿é€ä¿¡ï¼ˆ3ã¤ã®ç”Ÿç”£ãƒ©ã‚¤ãƒ³ï¼‰
        message_to_quality = self.send_message(
            to_agent="QualityAnalysis",
            message_type="production_data",
            data={
                "production_rate_line1": sensor_data['production_rate_line1'].tolist(),
                "production_rate_line2": sensor_data['production_rate_line2'].tolist(),
                "production_rate_line3": sensor_data['production_rate_line3'].tolist(),
                "quality_indicators": {
                    "temperature_variance_line1": float(sensor_data['temperature_line1'].var()),
                    "temperature_variance_line2": float(sensor_data['temperature_line2'].var()),
                    "temperature_variance_line3": float(sensor_data['temperature_line3'].var()),
                    "pressure_stability_pump1": float(sensor_data['pressure_pump1'].std()),
                    "pressure_stability_pump2": float(sensor_data['pressure_pump2'].std()),
                    "pressure_stability_pump3": float(sensor_data['pressure_pump3'].std()),
                    "humidity_average": float(sensor_data['humidity'].mean()),
                    "air_quality_average": float(sensor_data['air_quality'].mean())
                }
            },
            priority="normal"
        )
        
        self.status = "completed"
        return {
            "messages_sent": [message_to_process, message_to_anomaly, message_to_quality],
            "data": sensor_data
        }

class AnomalyDetectionAgent(AIAgent):
    """ç•°å¸¸æ¤œçŸ¥ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    def __init__(self):
        super().__init__("AnomalyDetection", "ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡ºã¨åˆ†æ")
        
    def process(self, sensor_data: pd.DataFrame) -> Dict:
        """ç•°å¸¸æ¤œçŸ¥ã¨ã‚¢ãƒ©ãƒ¼ãƒˆç”Ÿæˆ"""
        self.status = "processing"
        self.log_action("anomaly_detection", "ç•°å¸¸æ¤œçŸ¥ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Ÿè¡Œ")
        
        # ç•°å¸¸æ¤œçŸ¥ï¼ˆè¤‡æ•°ãƒ©ã‚¤ãƒ³ã®çµ±åˆï¼‰
        # æ¸©åº¦ç•°å¸¸ï¼ˆ3ã¤ã®ãƒ©ã‚¤ãƒ³ã‚’çµ±åˆï¼‰
        temp_line1_threshold = (sensor_data['temperature_line1'].mean() - 2*sensor_data['temperature_line1'].std(),
                               sensor_data['temperature_line1'].mean() + 2*sensor_data['temperature_line1'].std())
        temp_anomalies_line1 = sensor_data[(sensor_data['temperature_line1'] < temp_line1_threshold[0]) | 
                                           (sensor_data['temperature_line1'] > temp_line1_threshold[1])]
        
        temp_line2_threshold = (sensor_data['temperature_line2'].mean() - 2*sensor_data['temperature_line2'].std(),
                               sensor_data['temperature_line2'].mean() + 2*sensor_data['temperature_line2'].std())
        temp_anomalies_line2 = sensor_data[(sensor_data['temperature_line2'] < temp_line2_threshold[0]) | 
                                           (sensor_data['temperature_line2'] > temp_line2_threshold[1])]
        
        temp_line3_threshold = (sensor_data['temperature_line3'].mean() - 2*sensor_data['temperature_line3'].std(),
                               sensor_data['temperature_line3'].mean() + 2*sensor_data['temperature_line3'].std())
        temp_anomalies_line3 = sensor_data[(sensor_data['temperature_line3'] < temp_line3_threshold[0]) | 
                                           (sensor_data['temperature_line3'] > temp_line3_threshold[1])]
        
        # å…¨æ¸©åº¦ç•°å¸¸ã‚’çµ±åˆ
        temp_anomalies = pd.concat([temp_anomalies_line1, temp_anomalies_line2, temp_anomalies_line3]).drop_duplicates()
        
        # æŒ¯å‹•ç•°å¸¸ï¼ˆ3ã¤ã®ãƒ¢ãƒ¼ã‚¿ãƒ¼ã‚’çµ±åˆï¼‰
        vib_motor1_threshold = sensor_data['vibration_motor1'].mean() + 2*sensor_data['vibration_motor1'].std()
        vib_anomalies_motor1 = sensor_data[sensor_data['vibration_motor1'] > vib_motor1_threshold]
        
        vib_motor2_threshold = sensor_data['vibration_motor2'].mean() + 2*sensor_data['vibration_motor2'].std()
        vib_anomalies_motor2 = sensor_data[sensor_data['vibration_motor2'] > vib_motor2_threshold]
        
        vib_motor3_threshold = sensor_data['vibration_motor3'].mean() + 2*sensor_data['vibration_motor3'].std()
        vib_anomalies_motor3 = sensor_data[sensor_data['vibration_motor3'] > vib_motor3_threshold]
        
        # å…¨æŒ¯å‹•ç•°å¸¸ã‚’çµ±åˆ
        vib_anomalies = pd.concat([vib_anomalies_motor1, vib_anomalies_motor2, vib_anomalies_motor3]).drop_duplicates()
        
        anomalies_detected = len(temp_anomalies) + len(vib_anomalies)
        
        messages = []
        
        # ğŸ¤– LLMã§ç•°å¸¸åŸå› ã‚’åˆ†æï¼ˆå¸¸ã«å®Ÿè¡Œï¼‰
        llm_prompt = f"""ã‚ãªãŸã¯å·¥å ´ã®ç•°å¸¸æ¤œçŸ¥ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚

æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸:
- æ¸©åº¦ç•°å¸¸: {len(temp_anomalies)}ä»¶
- æŒ¯å‹•ç•°å¸¸: {len(vib_anomalies)}ä»¶
- ç·ç•°å¸¸æ•°: {anomalies_detected}ä»¶

ã‚»ãƒ³ã‚µãƒ¼çµ±è¨ˆ:
- Line 1å¹³å‡æ¸©åº¦: {sensor_data['temperature_line1'].mean():.2f}Â°C
- Line 2å¹³å‡æ¸©åº¦: {sensor_data['temperature_line2'].mean():.2f}Â°C
- Line 3å¹³å‡æ¸©åº¦: {sensor_data['temperature_line3'].mean():.2f}Â°C
- Motor 1å¹³å‡æŒ¯å‹•: {sensor_data['vibration_motor1'].mean():.3f}

{"ç•°å¸¸ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚" if anomalies_detected > 0 else "æ­£å¸¸ç¯„å›²å†…ã§ã™ã€‚"}è€ƒãˆã‚‰ã‚Œã‚‹åŸå› ã‚’2-3å€‹æŒ™ã’ã¦ã€ãã‚Œãã‚Œã®å¯¾ç­–ã‚’ç°¡æ½”ã«ææ¡ˆã—ã¦ãã ã•ã„ã€‚"""

        llm_analysis = call_ollama(
            llm_prompt,
            system_prompt="ã‚ãªãŸã¯å·¥å ´ç•°å¸¸æ¤œçŸ¥ã®å°‚é–€å®¶ã§ã™ã€‚åŸå› ã¨å¯¾ç­–ã‚’ç°¡æ½”ã«2-3å€‹ææ¡ˆã—ã¦ãã ã•ã„ã€‚"
        )
        
        self.log_action("llm_anomaly_analysis", f"LLMç•°å¸¸åˆ†æ: {llm_analysis[:100]}...")
        
        # ç•°å¸¸ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆ
        if anomalies_detected > 0:
            # è­¦å‘Šãƒ»é€šçŸ¥ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸å³åº§ã«ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡ï¼ˆé«˜å„ªå…ˆåº¦ï¼‰
            message_to_alert = self.send_message(
                to_agent="AlertNotification",
                message_type="anomaly_alert",
                data={
                    "severity": "high" if anomalies_detected > 5 else "medium",
                    "anomalies": {
                        "temperature_anomalies": len(temp_anomalies),
                        "vibration_anomalies": len(vib_anomalies)
                    },
                    "recommended_actions": ["immediate_inspection", "process_adjustment"],
                    "affected_equipment": ["line_2", "pump_3"],
                    "llm_root_cause_analysis": llm_analysis
                },
                priority="urgent"
            )
            messages.append(message_to_alert)
            
            # å“è³ªåˆ†æã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ç•°å¸¸æƒ…å ±ã‚’é€ä¿¡
            message_to_quality = self.send_message(
                to_agent="QualityAnalysis",
                message_type="anomaly_impact_data",
                data={
                    "anomaly_periods": temp_anomalies['timestamp'].astype(str).tolist() if len(temp_anomalies) > 0 else [],
                    "impact_severity": "high" if anomalies_detected > 5 else "medium",
                    "potential_quality_impact": True,
                    "llm_analysis": llm_analysis
                },
                priority="high"
            )
            messages.append(message_to_quality)
            
            # äºˆæ¸¬ä¿å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ãƒªã‚¹ã‚¯æƒ…å ±ã‚’é€ä¿¡
            message_to_maintenance = self.send_message(
                to_agent="PredictiveMaintenance",
                message_type="risk_indicator_update",
                data={
                    "vibration_trend": "increasing",
                    "temperature_instability": True,
                    "failure_risk_increase": 0.15
                },
                priority="high"
            )
            messages.append(message_to_maintenance)
        
        # ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
        message_to_process = self.send_message(
            to_agent="ProcessControl",
            message_type="anomaly_feedback",
            data={
                "anomaly_count": anomalies_detected,
                "control_effectiveness": "good" if anomalies_detected < 3 else "needs_improvement",
                "suggestions": ["tighten_temperature_control"] if len(temp_anomalies) > 2 else []
            },
            priority="normal"
        )
        messages.append(message_to_process)
        
        self.status = "completed"
        return {
            "messages_sent": messages,
            "anomalies": {
                "temp_anomalies": temp_anomalies,
                "vib_anomalies": vib_anomalies,
                "total_count": anomalies_detected
            }
        }

class QualityAnalysisAgent(AIAgent):
    """å“è³ªåˆ†æã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    def __init__(self):
        super().__init__("QualityAnalysis", "è£½å“å“è³ªã®åˆ†æã¨æ”¹å–„ææ¡ˆ")
        
    def process(self, production_data: Dict, anomaly_data: Dict = None) -> Dict:
        """å“è³ªåˆ†æã¨AIæ”¹å–„ææ¡ˆï¼ˆLLMçµ±åˆï¼‰"""
        self.status = "processing"
        self.log_action("quality_analysis", "å“è³ªæŒ‡æ¨™ã®è¨ˆç®—é–‹å§‹")
        
        # å“è³ªæŒ‡æ¨™è¨ˆç®—
        quality_score = np.random.uniform(92, 98)
        defect_rate = np.random.uniform(0.5, 2.5)
        
        # ç•°å¸¸ã®å½±éŸ¿ã‚’è€ƒæ…®
        if anomaly_data and anomaly_data.get('potential_quality_impact'):
            quality_score -= 2
            defect_rate += 0.5
        
        # ğŸ¤– LLMã§å“è³ªæ”¹å–„ç­–ã‚’ææ¡ˆ
        llm_prompt = f"""ã‚ãªãŸã¯è£½é€ å“è³ªç®¡ç†ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚

ç¾åœ¨ã®å“è³ªçŠ¶æ³:
- å“è³ªã‚¹ã‚³ã‚¢: {quality_score:.2f}ç‚¹ï¼ˆç›®æ¨™: 95ç‚¹ä»¥ä¸Šï¼‰
- ä¸è‰¯ç‡: {defect_rate:.2f}%
- ç”Ÿç”£é€Ÿåº¦: {production_data.get('rate', 95):.2f} units/min
- ç•°å¸¸æ¤œå‡º: {"ã‚ã‚Š" if anomaly_data and anomaly_data.get('potential_quality_impact') else "ãªã—"}

{"å“è³ªãŒç›®æ¨™ã‚’ä¸‹å›ã£ã¦ã„ã¾ã™ã€‚" if quality_score < 95 else "å“è³ªã¯è‰¯å¥½ã§ã™ã€‚"}å…·ä½“çš„ãªæ”¹å–„ç­–ã‚’3ã¤ææ¡ˆã—ã¦ãã ã•ã„ã€‚å„ææ¡ˆã¯ç°¡æ½”ã«1-2æ–‡ã§ã€‚"""

        llm_improvement = call_ollama(
            llm_prompt,
            system_prompt="ã‚ãªãŸã¯è£½é€ å“è³ªã®å°‚é–€å®¶ã§ã™ã€‚å®Ÿè¡Œå¯èƒ½ãªæ”¹å–„ç­–ã‚’3ã¤ã€ç°¡æ½”ã«ææ¡ˆã—ã¦ãã ã•ã„ã€‚"
        )
        
        self.log_action("llm_quality_improvement", f"LLMæ”¹å–„ææ¡ˆ: {llm_improvement[:100]}...")
        
        messages = []
        
        # å“è³ªã‚¹ã‚³ã‚¢ãŒä½ã„å ´åˆã€è¤‡æ•°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸é€šçŸ¥
        if quality_score < 95:
            # è­¦å‘Šãƒ»é€šçŸ¥ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ã‚¢ãƒ©ãƒ¼ãƒˆ
            message_to_alert = self.send_message(
                to_agent="AlertNotification",
                message_type="quality_alert",
                data={
                    "severity": "medium",
                    "quality_score": quality_score,
                    "defect_rate": defect_rate,
                    "trend": "declining",
                    "llm_improvement_plan": llm_improvement
                },
                priority="high"
            )
            messages.append(message_to_alert)
            
            # ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸æ”¹å–„è¦æ±‚
            message_to_process = self.send_message(
                to_agent="ProcessControl",
                message_type="quality_improvement_request",
                data={
                    "target_parameters": ["temperature_stability", "pressure_consistency"],
                    "required_improvement": 3.0,
                    "recommendations": [
                        "reduce_temperature_variance",
                        "optimize_heating_cycle"
                    ],
                    "llm_analysis": llm_improvement
                },
                priority="high"
            )
            messages.append(message_to_process)
        
        # ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸è¿½åŠ ãƒ‡ãƒ¼ã‚¿è¦æ±‚
        message_to_data = self.send_message(
            to_agent="DataCollection",
            message_type="request_quality_data",
            data={
                "data_points": ["product_dimensions", "surface_quality", "material_properties"],
                "sampling_frequency": "every_10_units"
            },
            priority="normal"
        )
        messages.append(message_to_data)
        
        # äºˆæ¸¬ä¿å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰æƒ…å ±ã‚’é€ä¿¡
        message_to_maintenance = self.send_message(
            to_agent="PredictiveMaintenance",
            message_type="quality_trend_data",
            data={
                "quality_degradation": quality_score < 95,
                "correlation_with_equipment": True,
                "maintenance_impact_prediction": "high"
            },
            priority="medium"
        )
        messages.append(message_to_maintenance)
        
        self.status = "completed"
        return {
            "messages_sent": messages,
            "analysis": {
                "quality_score": quality_score,
                "defect_rate": defect_rate
            }
        }

class AlertNotificationAgent(AIAgent):
    """è­¦å‘Šãƒ»é€šçŸ¥ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    def __init__(self):
        super().__init__("AlertNotification", "ã‚¢ãƒ©ãƒ¼ãƒˆç®¡ç†ã¨é€šçŸ¥é…ä¿¡")
        self.active_alerts = []
        
    def process(self, alert_data: Dict) -> Dict:
        """ã‚¢ãƒ©ãƒ¼ãƒˆå‡¦ç†ã¨é€šçŸ¥é…ä¿¡ï¼ˆLLMçµ±åˆï¼‰"""
        self.status = "processing"
        self.log_action("alert_processing", f"ã‚¢ãƒ©ãƒ¼ãƒˆå‡¦ç†: {alert_data.get('severity', 'unknown')}")
        
        alert_id = str(uuid.uuid4())
        self.active_alerts.append({
            "id": alert_id,
            "data": alert_data,
            "timestamp": datetime.now()
        })
        
        # ğŸ¤– LLMã§é€šçŸ¥ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ
        anomalies_info = alert_data.get('anomalies', {})
        quality_info = alert_data.get('quality_score', 'N/A')
        
        llm_prompt = f"""ã‚ãªãŸã¯å·¥å ´ç®¡ç†è€…ã¸ã®é€šçŸ¥ã‚’ä½œæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚

ã‚¢ãƒ©ãƒ¼ãƒˆæƒ…å ±:
- é‡è¦åº¦: {alert_data.get('severity', 'medium')}
- ç•°å¸¸æ¤œå‡ºæ•°: {anomalies_info if isinstance(anomalies_info, dict) else 'N/A'}
- å“è³ªã‚¹ã‚³ã‚¢: {quality_info}
- å½±éŸ¿ç¯„å›²: {alert_data.get('affected_equipment', [])}

ç®¡ç†è€…å‘ã‘ã«ã€çŠ¶æ³ã‚’ç°¡æ½”ã«èª¬æ˜ã—ã€æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’3ã¤ç®‡æ¡æ›¸ãã§ææ¡ˆã—ã¦ãã ã•ã„ã€‚å…¨ä½“ã§5æ–‡ä»¥å†…ã€‚"""

        llm_notification = call_ollama(
            llm_prompt,
            system_prompt="ã‚ãªãŸã¯å·¥å ´ç®¡ç†è€…ã¸ã®é€šçŸ¥ä½œæˆã®å°‚é–€å®¶ã§ã™ã€‚ç°¡æ½”æ˜ç­ã«ã€5æ–‡ä»¥å†…ã§çŠ¶æ³ã¨å¯¾ç­–ã‚’ä¼ãˆã¦ãã ã•ã„ã€‚"
        )
        
        self.log_action("llm_notification", f"LLMé€šçŸ¥æ–‡: {llm_notification[:100]}...")
        
        messages = []
        
        # é‡è¦åº¦ã«å¿œã˜ã¦è¤‡æ•°ã®é€šçŸ¥ãƒãƒ£ãƒãƒ«ã¸é…ä¿¡
        if alert_data.get('severity') == 'high' or alert_data.get('severity') == 'urgent':
            # ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ç·Šæ€¥åœæ­¢è¦æ±‚
            message_to_process = self.send_message(
                to_agent="ProcessControl",
                message_type="emergency_action_required",
                data={
                    "alert_id": alert_id,
                    "action": "reduce_production_rate",
                    "reason": alert_data.get('anomalies', {}),
                    "llm_notification": llm_notification
                },
                priority="urgent"
            )
            messages.append(message_to_process)
            
            # äºˆæ¸¬ä¿å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ç·Šæ€¥ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹è¦æ±‚
            message_to_maintenance = self.send_message(
                to_agent="PredictiveMaintenance",
                message_type="urgent_maintenance_request",
                data={
                    "alert_id": alert_id,
                    "equipment": alert_data.get('affected_equipment', []),
                    "urgency": "immediate",
                    "llm_notification": llm_notification
                },
                priority="urgent"
            )
            messages.append(message_to_maintenance)
        
        # ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ç›£è¦–å¼·åŒ–è¦æ±‚
        message_to_data = self.send_message(
            to_agent="DataCollection",
            message_type="increase_monitoring",
            data={
                "target_sensors": ["temperature", "vibration", "pressure"],
                "duration": "30_minutes",
                "sampling_rate": "maximum",
                "alert_context": llm_notification
            },
            priority="high"
        )
        messages.append(message_to_data)
        
        self.status = "completed"
        return {
            "messages_sent": messages,
            "alert_id": alert_id,
            "notifications_sent": ["email", "slack", "sms"] if alert_data.get('severity') == 'high' else ["email"]
        }

class PredictiveMaintenanceAgent(AIAgent):
    """äºˆæ¸¬ä¿å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    def __init__(self):
        super().__init__("PredictiveMaintenance", "æ•…éšœäºˆæ¸¬ã¨ä¿å…¨è¨ˆç”»")
        self.equipment_status = {}
        
    def process(self, risk_data: Dict) -> Dict:
        """æ•…éšœäºˆæ¸¬ã¨ä¿å…¨è¨ˆç”»ã®ç”Ÿæˆï¼ˆLLMçµ±åˆï¼‰"""
        self.status = "processing"
        self.log_action("predictive_analysis", "æ•…éšœãƒªã‚¹ã‚¯åˆ†æé–‹å§‹")
        
        # ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢è¨ˆç®—
        base_risk = np.random.uniform(20, 40)
        if risk_data.get('vibration_trend') == 'increasing':
            base_risk += 15
        if risk_data.get('temperature_instability'):
            base_risk += 10
        
        risk_score = min(100, base_risk)
        days_to_maintenance = max(7, int(30 - risk_score / 3))
        
        # ğŸ¤– LLMã§ä¿å…¨è¨ˆç”»ã‚’ç”Ÿæˆ
        llm_prompt = f"""ã‚ãªãŸã¯è¨­å‚™ä¿å…¨ã®å°‚é–€å®¶ã§ã™ã€‚

äºˆæ¸¬çµæœ:
- ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢: {risk_score:.1f}%
- æ¨å¥¨ä¿å…¨ã¾ã§: {days_to_maintenance}æ—¥
- æŒ¯å‹•ãƒˆãƒ¬ãƒ³ãƒ‰: {risk_data.get('vibration_trend', 'stable')}
- æ¸©åº¦ä¸å®‰å®š: {"ã‚ã‚Š" if risk_data.get('temperature_instability') else "ãªã—"}
- å¯¾è±¡è¨­å‚™: pump_3, conveyor_1

{"ãƒªã‚¹ã‚¯ãŒé«˜ã„ã§ã™ã€‚" if risk_score > 60 else "ãƒªã‚¹ã‚¯ã¯ä¸­ç¨‹åº¦ã§ã™ã€‚"}å…·ä½“çš„ãªä¿å…¨è¨ˆç”»ã‚’3ã‚¹ãƒ†ãƒƒãƒ—ã§ææ¡ˆã—ã¦ãã ã•ã„ã€‚å„ã‚¹ãƒ†ãƒƒãƒ—ã¯ç°¡æ½”ã«1æ–‡ã§ã€‚"""

        llm_maintenance_plan = call_ollama(
            llm_prompt,
            system_prompt="ã‚ãªãŸã¯è¨­å‚™ä¿å…¨ã®å°‚é–€å®¶ã§ã™ã€‚å®Ÿè¡Œå¯èƒ½ãªä¿å…¨è¨ˆç”»ã‚’3ã‚¹ãƒ†ãƒƒãƒ—ã€ç°¡æ½”ã«ææ¡ˆã—ã¦ãã ã•ã„ã€‚"
        )
        
        self.log_action("llm_maintenance_plan", f"LLMä¿å…¨è¨ˆç”»: {llm_maintenance_plan[:100]}...")
        
        messages = []
        
        # ãƒªã‚¹ã‚¯ãŒé«˜ã„å ´åˆã€è¤‡æ•°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸è­¦å‘Š
        if risk_score > 60:
            # è­¦å‘Šãƒ»é€šçŸ¥ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ä¿å…¨ã‚¢ãƒ©ãƒ¼ãƒˆ
            message_to_alert = self.send_message(
                to_agent="AlertNotification",
                message_type="maintenance_alert",
                data={
                    "severity": "high" if risk_score > 80 else "medium",
                    "risk_score": risk_score,
                    "days_to_maintenance": days_to_maintenance,
                    "equipment": ["pump_3", "conveyor_1"],
                    "llm_maintenance_plan": llm_maintenance_plan
                },
                priority="high"
            )
            messages.append(message_to_alert)
            
            # ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸è² è·è»½æ¸›è¦æ±‚
            message_to_process = self.send_message(
                to_agent="ProcessControl",
                message_type="reduce_equipment_load",
                data={
                    "reason": "high_failure_risk",
                    "target_equipment": ["pump_3"],
                    "recommended_load_reduction": 0.2,
                    "llm_plan": llm_maintenance_plan
                },
                priority="high"
            )
            messages.append(message_to_process)
        
        # å“è³ªåˆ†æã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ä¿å…¨å½±éŸ¿ã‚’é€šçŸ¥
        message_to_quality = self.send_message(
            to_agent="QualityAnalysis",
            message_type="maintenance_schedule_update",
            data={
                "scheduled_maintenance": {
                    "date": (datetime.now() + timedelta(days=days_to_maintenance)).isoformat(),
                    "duration": "4_hours",
                    "affected_lines": ["line_2"]
                },
                "production_impact": "medium",
                "llm_maintenance_plan": llm_maintenance_plan
            },
            priority="normal"
        )
        messages.append(message_to_quality)
        
        # ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ç”¨ãƒ‡ãƒ¼ã‚¿è¦æ±‚
        message_to_data = self.send_message(
            to_agent="DataCollection",
            message_type="request_historical_data",
            data={
                "time_range": "last_30_days",
                "sensors": ["vibration", "temperature", "operating_hours"],
                "purpose": "model_training",
                "llm_context": llm_maintenance_plan
            },
            priority="low"
        )
        messages.append(message_to_data)
        
        self.status = "completed"
        return {
            "messages_sent": messages,
            "prediction": {
                "risk_score": risk_score,
                "days_to_maintenance": days_to_maintenance
            }
        }

# ================================
# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
# ================================
if 'agents' not in st.session_state:
    st.session_state.agents = {
        'ProcessControl': ProcessControlAgent(),
        'DataCollection': DataCollectionAgent(),
        'AnomalyDetection': AnomalyDetectionAgent(),
        'QualityAnalysis': QualityAnalysisAgent(),
        'AlertNotification': AlertNotificationAgent(),
        'PredictiveMaintenance': PredictiveMaintenanceAgent()
    }

if 'communication_log' not in st.session_state:
    st.session_state.communication_log = []

if 'total_traffic_kb' not in st.session_state:
    st.session_state.total_traffic_kb = 0

# æ‰¿èªãƒ•ãƒ­ãƒ¼ç”¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹
if 'workflow_step' not in st.session_state:
    st.session_state.workflow_step = 0

if 'workflow_data' not in st.session_state:
    st.session_state.workflow_data = {}

if 'waiting_for_approval' not in st.session_state:
    st.session_state.waiting_for_approval = False

# ================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†ãƒ•ãƒ­ãƒ¼ï¼ˆã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—æ‰¿èªï¼‰
# ================================
def execute_workflow_step(step: int):
    """æŒ‡å®šã•ã‚ŒãŸã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œ"""
    
    if step == 0:
        # ã‚¹ãƒ†ãƒƒãƒ—0: åˆæœŸåŒ–
        st.session_state.workflow_data = {}
        st.session_state.workflow_step = 1
        return "ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é–‹å§‹æº–å‚™å®Œäº†"
    
    elif step == 1:
        # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
        data_agent = st.session_state.agents['DataCollection']
        data_result = data_agent.process()
        sensor_data = data_result['data']
        
        # é€šä¿¡ãƒ­ã‚°ã«è¨˜éŒ²
        for msg in data_result['messages_sent']:
            st.session_state.communication_log.append(msg)
            st.session_state.total_traffic_kb += msg['size_kb']
        
        # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ãŸã‚ã«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        st.session_state.workflow_data['sensor_data'] = sensor_data
        st.session_state.workflow_data['data_messages'] = data_result['messages_sent']
        
        return f"âœ… ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†: {len(data_result['messages_sent'])}ä»¶ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡"
    
    elif step == 2:
        # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
        process_agent = st.session_state.agents['ProcessControl']
        sensor_data = st.session_state.workflow_data['sensor_data']
        process_result = process_agent.process(sensor_data)
        
        for msg in process_result['messages_sent']:
            st.session_state.communication_log.append(msg)
            st.session_state.total_traffic_kb += msg['size_kb']
        
        st.session_state.workflow_data['process_result'] = process_result
        
        return f"âœ… ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡å®Œäº†: {len(process_result['messages_sent'])}ä»¶ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡"
    
    elif step == 3:
        # ã‚¹ãƒ†ãƒƒãƒ—3: ç•°å¸¸æ¤œçŸ¥ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
        anomaly_agent = st.session_state.agents['AnomalyDetection']
        sensor_data = st.session_state.workflow_data['sensor_data']
        anomaly_result = anomaly_agent.process(sensor_data)
        
        for msg in anomaly_result['messages_sent']:
            st.session_state.communication_log.append(msg)
            st.session_state.total_traffic_kb += msg['size_kb']
        
        st.session_state.workflow_data['anomaly_result'] = anomaly_result
        
        anomaly_count = anomaly_result['anomalies']['total_count']
        return f"âœ… ç•°å¸¸æ¤œçŸ¥å®Œäº†: {anomaly_count}ä»¶ã®ç•°å¸¸æ¤œå‡ºã€{len(anomaly_result['messages_sent'])}ä»¶ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡"
    
    elif step == 4:
        # ã‚¹ãƒ†ãƒƒãƒ—4: å“è³ªåˆ†æã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
        quality_agent = st.session_state.agents['QualityAnalysis']
        sensor_data = st.session_state.workflow_data['sensor_data']
        anomaly_result = st.session_state.workflow_data['anomaly_result']
        
        # 3ã¤ã®ãƒ©ã‚¤ãƒ³ã®å¹³å‡ç”Ÿç”£é€Ÿåº¦ã‚’è¨ˆç®—
        avg_production_rate = (sensor_data['production_rate_line1'].mean() + 
                               sensor_data['production_rate_line2'].mean() + 
                               sensor_data['production_rate_line3'].mean()) / 3
        
        quality_result = quality_agent.process(
            production_data={'rate': avg_production_rate},
            anomaly_data={'potential_quality_impact': anomaly_result['anomalies']['total_count'] > 3}
        )
        
        for msg in quality_result['messages_sent']:
            st.session_state.communication_log.append(msg)
            st.session_state.total_traffic_kb += msg['size_kb']
        
        st.session_state.workflow_data['quality_result'] = quality_result
        
        quality_score = quality_result['analysis']['quality_score']
        return f"âœ… å“è³ªåˆ†æå®Œäº†: å“è³ªã‚¹ã‚³ã‚¢ {quality_score:.1f}ç‚¹ã€{len(quality_result['messages_sent'])}ä»¶ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡"
    
    elif step == 5:
        # ã‚¹ãƒ†ãƒƒãƒ—5: è­¦å‘Šãƒ»é€šçŸ¥ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆç•°å¸¸ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
        anomaly_result = st.session_state.workflow_data['anomaly_result']
        
        if anomaly_result['anomalies']['total_count'] > 0:
            alert_agent = st.session_state.agents['AlertNotification']
            alert_result = alert_agent.process({
                'severity': 'high' if anomaly_result['anomalies']['total_count'] > 5 else 'medium',
                'anomalies': anomaly_result['anomalies'],
                'affected_equipment': ['line_2', 'pump_3']
            })
            
            for msg in alert_result['messages_sent']:
                st.session_state.communication_log.append(msg)
                st.session_state.total_traffic_kb += msg['size_kb']
            
            st.session_state.workflow_data['alert_result'] = alert_result
            
            return f"âš ï¸ è­¦å‘Šãƒ»é€šçŸ¥å®Œäº†: ã‚¢ãƒ©ãƒ¼ãƒˆID {alert_result['alert_id']}ã€{len(alert_result['messages_sent'])}ä»¶ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡"
        else:
            return "âœ… ç•°å¸¸ãªã—: è­¦å‘Šãƒ»é€šçŸ¥ã‚¹ã‚­ãƒƒãƒ—"
    
    elif step == 6:
        # ã‚¹ãƒ†ãƒƒãƒ—6: äºˆæ¸¬ä¿å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
        maintenance_agent = st.session_state.agents['PredictiveMaintenance']
        anomaly_result = st.session_state.workflow_data['anomaly_result']
        
        maintenance_result = maintenance_agent.process({
            'vibration_trend': 'increasing' if np.random.random() > 0.5 else 'stable',
            'temperature_instability': anomaly_result['anomalies']['total_count'] > 2
        })
        
        for msg in maintenance_result['messages_sent']:
            st.session_state.communication_log.append(msg)
            st.session_state.total_traffic_kb += msg['size_kb']
        
        st.session_state.workflow_data['maintenance_result'] = maintenance_result
        
        risk_score = maintenance_result['prediction']['risk_score']
        days = maintenance_result['prediction']['days_to_maintenance']
        return f"âœ… äºˆæ¸¬ä¿å…¨å®Œäº†: ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ {risk_score:.1f}%ã€æ¨å¥¨ä¿å…¨ã¾ã§{days}æ—¥ã€{len(maintenance_result['messages_sent'])}ä»¶ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡"
    
    elif step == 7:
        # ã‚¹ãƒ†ãƒƒãƒ—7: å®Œäº†
        return "ğŸ‰ å…¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Œäº†ï¼"
    
    return "ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œå®Œäº†"

# ================================
# UIæ§‹ç¯‰
# ================================

# ãƒ˜ãƒƒãƒ€ãƒ¼
st.markdown('<div class="main-header">ğŸ¤– AI Multi-Agent Communication System</div>', unsafe_allow_html=True)

# ã‚µã‚¤ãƒ‰ãƒãƒ¼
with st.sidebar:
    st.header("âš™ï¸ ã‚·ã‚¹ãƒ†ãƒ åˆ¶å¾¡")
    
    # ğŸ¤– Ollamaã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º
    ollama_status = check_ollama_status()
    
    if ollama_status['status'] == 'running' and ollama_status['llama_installed']:
        st.success(f"âœ… Ollamaç¨¼åƒä¸­")
        st.caption(f"ğŸ¦™ Model: {OLLAMA_MODEL}")
    elif ollama_status['status'] == 'running' and not ollama_status['llama_installed']:
        st.warning("âš ï¸ Ollamaç¨¼åƒä¸­ï¼ˆLlamaãƒ¢ãƒ‡ãƒ«æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰")
        st.caption(f"å®Ÿè¡Œ: `ollama pull {OLLAMA_MODEL}`")
    else:
        st.error("âŒ Ollamaæœªèµ·å‹•")
        st.caption("å®Ÿè¡Œ: `ollama serve`")
    
    st.markdown("---")
    
    # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¹ãƒ†ãƒƒãƒ—è¡¨ç¤º
    step_names = [
        "å¾…æ©Ÿä¸­",
        "1ï¸âƒ£ ãƒ‡ãƒ¼ã‚¿åé›†",
        "2ï¸âƒ£ ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ ğŸ¤–",
        "3ï¸âƒ£ ç•°å¸¸æ¤œçŸ¥ ğŸ¤–",
        "4ï¸âƒ£ å“è³ªåˆ†æ ğŸ¤–",
        "5ï¸âƒ£ è­¦å‘Šãƒ»é€šçŸ¥ ğŸ¤–",
        "6ï¸âƒ£ äºˆæ¸¬ä¿å…¨ ğŸ¤–",
        "âœ… å®Œäº†"
    ]
    
    current_step = st.session_state.workflow_step
    st.markdown(f"### ğŸ§  Agentã®å®Ÿè¡Œ")
    st.info(f"{step_names[current_step]}")
    
    # é€²è¡ŒçŠ¶æ³ãƒãƒ¼
    if current_step > 0:
        progress = (current_step - 1) / 6
        st.progress(progress)
        st.caption(f"é€²æ—: {int(progress * 100)}%")
    
    st.markdown("---")
    
    # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é–‹å§‹ãƒœã‚¿ãƒ³
    if current_step == 0:
        if st.button("ğŸš€ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é–‹å§‹", type="primary", use_container_width=True):
            execute_workflow_step(0)
            st.session_state.workflow_step = 1
            st.session_state.waiting_for_approval = True
            st.rerun()
    
    # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸é€²ã‚€ãƒœã‚¿ãƒ³
    elif 1 <= current_step <= 6:
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("â–¶ï¸ æ¬¡ã¸", type="primary", use_container_width=True):
                result_message = execute_workflow_step(current_step)
                st.session_state.workflow_step += 1
                st.session_state.waiting_for_approval = True
                st.success(result_message)
                st.rerun()
        
        with col2:
            if st.button("â¸ï¸ ä¸€æ™‚åœæ­¢", use_container_width=True):
                st.session_state.waiting_for_approval = True
                st.warning("ä¸€æ™‚åœæ­¢ä¸­")
    
    # å®Œäº†å¾Œã®ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³
    elif current_step == 7:
        if st.button("ğŸ”„ æœ€åˆã‹ã‚‰", type="primary", use_container_width=True):
            st.session_state.workflow_step = 0
            st.session_state.workflow_data = {}
            st.session_state.waiting_for_approval = False
            st.rerun()
    
    st.markdown("---")
    
    # é€šä¿¡ãƒ­ã‚°ã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³
    if st.button("ğŸ—‘ï¸ é€šä¿¡ãƒ­ã‚°ã‚¯ãƒªã‚¢", use_container_width=True):
        st.session_state.communication_log = []
        st.session_state.total_traffic_kb = 0
        st.rerun()
    
    st.markdown("---")
    st.header("ğŸ“Š é€šä¿¡çµ±è¨ˆ")
    
    st.metric("ç·é€šä¿¡é‡", f"{st.session_state.total_traffic_kb:.2f} KB")
    st.metric("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°", len(st.session_state.communication_log))
    
    if len(st.session_state.communication_log) > 0:
        # é€šä¿¡é‡ä¸Šä½ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—
        df_log = pd.DataFrame(st.session_state.communication_log)
        traffic_by_type = df_log.groupby('type')['size_kb'].sum().sort_values(ascending=False)
        
        st.markdown("### ğŸ“¡ é€šä¿¡é‡TOP3")
        for i, (msg_type, size) in enumerate(traffic_by_type.head(3).items(), 1):
            traffic_class = "traffic-high" if size > 5 else "traffic-medium" if size > 1 else "traffic-low"
            st.markdown(f"""
            <div class="{traffic_class}">
                {i}. {msg_type}<br>
                {size:.2f} KB
            </div>
            """, unsafe_allow_html=True)
    
    st.markdown("---")
    st.header("ğŸ¤– ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçŠ¶æ…‹")
    
    for name, agent in st.session_state.agents.items():
        status_emoji = "ğŸŸ¢" if agent.status == "completed" else "ğŸŸ¡" if agent.status == "processing" else "âšª"
        st.markdown(f"{status_emoji} **{name}**<br><small>{agent.role}</small>", unsafe_allow_html=True)

# ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ - ã‚¿ãƒ–æ§‹æˆ
tab1, tab2, tab3, tab4 = st.tabs([
    "ğŸ”„ é€šä¿¡ãƒ•ãƒ­ãƒ¼",
    "ğŸ’¬ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è©³ç´°",
    "ğŸ“Š é€šä¿¡é‡åˆ†æ",
    "ğŸ’° AIãƒˆãƒ¼ã‚¯ãƒ³æ•°"
])

# ã‚¿ãƒ–1: é€šä¿¡ãƒ•ãƒ­ãƒ¼
with tab1:
    st.header("ğŸ”„ AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“é€šä¿¡ãƒ•ãƒ­ãƒ¼")
    
    st.markdown("""
    ### ğŸ“‹ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®èª¬æ˜
    
    ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€6ã¤ã®AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒ**äººé–“ã®æŒ‡ç¤ºãªã—**ã«è‡ªå¾‹çš„ã«é€šä¿¡ã—ã€
    å·¥å ´ã®è£½é€ ãƒ—ãƒ­ã‚»ã‚¹ã‚’æœ€é©åŒ–ã—ã¾ã™ã€‚
    
    **é€šä¿¡ã®ç‰¹å¾´:**
    - âœ… ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåŒå£«ãŒè‡ªå‹•çš„ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸äº¤æ›
    - âœ… å„ªå…ˆåº¦ã«å¿œã˜ãŸå‡¦ç†é †åºã®è‡ªå‹•èª¿æ•´
    - âœ… ç•°å¸¸æ¤œå‡ºæ™‚ã®è‡ªå‹•ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    - âœ… ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ã«ã‚ˆã‚‹ç¶™ç¶šçš„æ”¹å–„
    """)
    
    # ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ
    st.markdown("### ğŸ”€ é€šä¿¡ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ")
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        # Mermaidé¢¨ã®ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ï¼‰
        st.markdown("""
        ```
        ãƒ‡ãƒ¼ã‚¿åé›† â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
            â†“ (ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿é€ä¿¡: å¤§å®¹é‡)           â†“
            â†“                                        â†“
        ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡ â†â†’ ç•°å¸¸æ¤œçŸ¥ â†â†’ å“è³ªåˆ†æ        â†“
            â†“           â†“           â†“                â†“
            â†“           â†“ (ç·Šæ€¥)    â†“                â†“
            â†“           â†“           â†“                â†“
            â†“        è­¦å‘Šãƒ»é€šçŸ¥ â†â”€â”€â”€â”˜                â†“
            â†“           â†“                            â†“
            â†“           â†“ (ç·Šæ€¥ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹è¦æ±‚)     â†“
            â†“           â†“                            â†“
            â””â”€â”€â”€â”€â”€â”€â”€â†’ äºˆæ¸¬ä¿å…¨ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
                    (ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯)
                        â†“
                    å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
        ```
        """)
    
    with col2:
        st.markdown("""
        **é€šä¿¡é‡ãŒå¤§ãã„ãƒã‚¤ãƒ³ãƒˆ:**
        
        ğŸ”´ **å¤§** (>10KB)
        - ãƒ‡ãƒ¼ã‚¿åé›†â†’ç•°å¸¸æ¤œçŸ¥
          (ç”Ÿãƒ‡ãƒ¼ã‚¿è»¢é€)
        
        ğŸŸ¡ **ä¸­** (1-10KB)
        - ç•°å¸¸æ¤œçŸ¥â†’è­¦å‘Šé€šçŸ¥
          (è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ)
        
        ğŸŸ¢ **å°** (<1KB)
        - åˆ¶å¾¡ã‚³ãƒãƒ³ãƒ‰
        - ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°
        """)
    
    st.markdown("---")
    
    # æœ€æ–°ã®é€šä¿¡ãƒ•ãƒ­ãƒ¼è¡¨ç¤º
    if len(st.session_state.communication_log) > 0:
        st.subheader("ğŸ“¨ æœ€æ–°ã®é€šä¿¡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ (ç›´è¿‘10ä»¶)")
        
        recent_messages = st.session_state.communication_log[-10:]
        
        for msg in reversed(recent_messages):
            timestamp = msg['timestamp'].strftime('%H:%M:%S')
            size_class = "traffic-high" if msg['size_kb'] > 5 else "traffic-medium" if msg['size_kb'] > 1 else "traffic-low"
            priority_emoji = "ğŸ”´" if msg['priority'] == "urgent" else "ğŸŸ " if msg['priority'] == "high" else "ğŸŸ¢"
            
            st.markdown(f"""
            <div class="message-flow">
                <div style="flex: 1;">
                    <strong>{msg['from']}</strong> â†’ <strong>{msg['to']}</strong><br>
                    <small>{timestamp} | {msg['type']}</small>
                </div>
                <div style="text-align: right;">
                    {priority_emoji} <span class="{size_class}">{msg['size_kb']:.2f} KB</span>
                </div>
            </div>
            """, unsafe_allow_html=True)

# ã‚¿ãƒ–2: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è©³ç´°
with tab2:
    st.header("ğŸ’¬ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è©³ç´°ãƒ­ã‚°")
    
    if len(st.session_state.communication_log) > 0:
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        col1, col2, col3 = st.columns(3)
        
        with col1:
            unique_senders = list(set([msg['from'] for msg in st.session_state.communication_log]))
            filter_sender = st.selectbox("é€ä¿¡å…ƒãƒ•ã‚£ãƒ«ã‚¿ãƒ¼", ["ã™ã¹ã¦"] + unique_senders)
        
        with col2:
            unique_receivers = list(set([msg['to'] for msg in st.session_state.communication_log]))
            filter_receiver = st.selectbox("é€ä¿¡å…ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼", ["ã™ã¹ã¦"] + unique_receivers)
        
        with col3:
            filter_priority = st.selectbox("å„ªå…ˆåº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼", ["ã™ã¹ã¦", "urgent", "high", "normal", "low"])
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨
        filtered_messages = st.session_state.communication_log
        
        if filter_sender != "ã™ã¹ã¦":
            filtered_messages = [msg for msg in filtered_messages if msg['from'] == filter_sender]
        
        if filter_receiver != "ã™ã¹ã¦":
            filtered_messages = [msg for msg in filtered_messages if msg['to'] == filter_receiver]
        
        if filter_priority != "ã™ã¹ã¦":
            filtered_messages = [msg for msg in filtered_messages if msg['priority'] == filter_priority]
        
        st.markdown(f"### ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœ: {len(filtered_messages)} ä»¶")
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è©³ç´°è¡¨ç¤º
        for i, msg in enumerate(reversed(filtered_messages[-20:]), 1):
            with st.expander(f"#{i} | {msg['from']} â†’ {msg['to']} | {msg['type']} | {msg['timestamp'].strftime('%H:%M:%S')}"):
                col1, col2 = st.columns([1, 2])
                
                with col1:
                    st.markdown(f"""
                    **ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ID:** `{msg['id'][:8]}...`  
                    **å„ªå…ˆåº¦:** `{msg['priority']}`  
                    **ã‚µã‚¤ã‚º:** `{msg['size_kb']:.2f} KB`  
                    **ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—:** `{msg['timestamp']}`
                    """)
                
                with col2:
                    st.markdown("**ãƒ‡ãƒ¼ã‚¿ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰:**")
                    st.json(msg['data'])
    else:
        st.info("ğŸ‘† ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã€Œãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„")

# ã‚¿ãƒ–3: é€šä¿¡é‡åˆ†æ
with tab3:
    st.header("ğŸ“Š é€šä¿¡é‡åˆ†æ")
    
    if len(st.session_state.communication_log) > 0:
        df_log = pd.DataFrame(st.session_state.communication_log)
        
        col1, col2 = st.columns(2)
        
        with col1:
            # é€ä¿¡å…ƒåˆ¥é€šä¿¡é‡
            traffic_by_sender = df_log.groupby('from')['size_kb'].sum().sort_values(ascending=False)
            
            fig_sender = px.bar(
                x=traffic_by_sender.values,
                y=traffic_by_sender.index,
                orientation='h',
                title="ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥é€ä¿¡é‡",
                labels={'x': 'é€šä¿¡é‡ (KB)', 'y': 'ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ'},
                color=traffic_by_sender.values,
                color_continuous_scale='RdYlGn_r'
            )
            fig_sender.update_layout(height=400, showlegend=False)
            st.plotly_chart(fig_sender, use_container_width=True)
        
        with col2:
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—åˆ¥é€šä¿¡é‡
            traffic_by_type = df_log.groupby('type')['size_kb'].sum().sort_values(ascending=False)
            
            fig_type = px.pie(
                values=traffic_by_type.values,
                names=traffic_by_type.index,
                title="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—åˆ¥é€šä¿¡é‡",
                hole=0.4
            )
            fig_type.update_layout(height=400)
            st.plotly_chart(fig_type, use_container_width=True)
        
        # é€šä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        st.subheader("ğŸ” é€šä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # å„ªå…ˆåº¦åˆ¥ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°
            priority_counts = df_log['priority'].value_counts()
            
            fig_priority = px.bar(
                x=priority_counts.index,
                y=priority_counts.values,
                title="å„ªå…ˆåº¦åˆ¥ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°",
                labels={'x': 'å„ªå…ˆåº¦', 'y': 'ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°'},
                color=priority_counts.index,
                color_discrete_map={
                    'urgent': '#dc3545',
                    'high': '#ffc107',
                    'normal': '#17a2b8',
                    'low': '#28a745'
                }
            )
            fig_priority.update_layout(height=350)
            st.plotly_chart(fig_priority, use_container_width=True)
        
        with col2:
            # é€šä¿¡ãƒšã‚¢åˆ†æ
            df_log['pair'] = df_log['from'] + ' â†’ ' + df_log['to']
            pair_counts = df_log['pair'].value_counts().head(10)
            
            fig_pair = px.bar(
                x=pair_counts.values,
                y=pair_counts.index,
                orientation='h',
                title="é€šä¿¡ãƒšã‚¢TOP10",
                labels={'x': 'ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°', 'y': 'é€šä¿¡ãƒšã‚¢'}
            )
            fig_pair.update_layout(height=350)
            st.plotly_chart(fig_pair, use_container_width=True)
        
        # é€šä¿¡é‡ã®å¤šã„ãƒã‚¤ãƒ³ãƒˆ
        st.subheader("ğŸ”´ é€šä¿¡é‡ãŒå¤šã„ãƒã‚¤ãƒ³ãƒˆ")
        
        heavy_messages = df_log[df_log['size_kb'] > 5].sort_values('size_kb', ascending=False)
        
        if len(heavy_messages) > 0:
            st.dataframe(
                heavy_messages[['from', 'to', 'type', 'size_kb', 'priority', 'timestamp']].head(10),
                use_container_width=True
            )
            
            st.markdown("""
            **å¤§å®¹é‡é€šä¿¡ã®ç†ç”±:**
            - `sensor_data_batch`: ã‚»ãƒ³ã‚µãƒ¼ç”Ÿãƒ‡ãƒ¼ã‚¿ã®ä¸€æ‹¬è»¢é€ï¼ˆ100ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ Ã— 5ã‚»ãƒ³ã‚µãƒ¼ï¼‰
            - `anomaly_alert`: ç•°å¸¸æ¤œçŸ¥çµæœã®è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãç•°å¸¸ãƒªã‚¹ãƒˆï¼‰
            - `request_historical_data`: éå»ãƒ‡ãƒ¼ã‚¿ã®è¦æ±‚ï¼ˆ30æ—¥åˆ†ã®å±¥æ­´ï¼‰
            """)
        else:
            st.info("å¤§å®¹é‡é€šä¿¡ï¼ˆ>5KBï¼‰ã¯æ¤œå‡ºã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # ğŸ“Š é€šä¿¡é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚·ãƒŠãƒªã‚ª
        st.subheader("ğŸ“Š é€šä¿¡é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚·ãƒŠãƒªã‚ª")
        
        # ç¾åœ¨ã®é€šä¿¡é‡
        current_total_kb = df_log['size_kb'].sum()
        current_sensors = 20
        current_datapoints = 1000
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆæ•°ã«æ¯”ä¾‹ï¼‰
        scaling_scenarios = pd.DataFrame({
            'ã‚·ãƒŠãƒªã‚ª': ['ç¾åœ¨', 'ä¸­è¦æ¨¡', 'å¤§è¦æ¨¡', 'è¶…å¤§è¦æ¨¡'],
            'ã‚»ãƒ³ã‚µãƒ¼æ•°': [20, 50, 100, 500],
            'ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ': [1000, 5000, 10000, 50000],
            'äºˆæƒ³é€šä¿¡é‡(KB)': [
                current_total_kb,
                current_total_kb * (50/20) * (5000/1000),
                current_total_kb * (100/20) * (10000/1000),
                current_total_kb * (500/20) * (50000/1000)
            ]
        })
        
        # MBå˜ä½ã‚‚è¿½åŠ 
        scaling_scenarios['äºˆæƒ³é€šä¿¡é‡(MB)'] = scaling_scenarios['äºˆæƒ³é€šä¿¡é‡(KB)'] / 1024
        
        st.markdown("""
        **ä»®å®š:**
        - ã‚»ãƒ³ã‚µãƒ¼æ•°ã¨ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆæ•°ã«æ¯”ä¾‹ã—ã¦é€šä¿¡é‡ãŒå¢—åŠ 
        - ç¾åœ¨ã®æ§‹æˆ: 20ã‚»ãƒ³ã‚µãƒ¼ Ã— 1,000ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ
        """)
        
        st.dataframe(
            scaling_scenarios[['ã‚·ãƒŠãƒªã‚ª', 'ã‚»ãƒ³ã‚µãƒ¼æ•°', 'ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ', 'äºˆæƒ³é€šä¿¡é‡(KB)', 'äºˆæƒ³é€šä¿¡é‡(MB)']],
            use_container_width=True,
            hide_index=True
        )
        
        # ã‚°ãƒ©ãƒ•åŒ–
        fig_scaling = px.bar(
            scaling_scenarios,
            x='ã‚·ãƒŠãƒªã‚ª',
            y='äºˆæƒ³é€šä¿¡é‡(MB)',
            title='ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚·ãƒŠãƒªã‚ªåˆ¥ã®äºˆæƒ³é€šä¿¡é‡',
            text='äºˆæƒ³é€šä¿¡é‡(MB)',
            color='äºˆæƒ³é€šä¿¡é‡(MB)',
            color_continuous_scale='Blues'
        )
        
        fig_scaling.update_traces(texttemplate='%{text:.2f} MB', textposition='outside')
        fig_scaling.update_layout(height=400, showlegend=False)
        
        st.plotly_chart(fig_scaling, use_container_width=True)
    
    else:
        st.info("ğŸ‘† ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã€Œãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„")

# ã‚¿ãƒ–4: AIãƒˆãƒ¼ã‚¯ãƒ³æ•°
with tab4:
    st.header("ğŸ’° AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ãƒˆãƒ¼ã‚¯ãƒ³æ•° & ã‚³ã‚¹ãƒˆåˆ†æ")
    
    st.markdown("""
    ### ğŸ“Š æ¦‚è¦
    
    AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒæ‰±ã†IoTãƒ‡ãƒ¼ã‚¿é‡ãŒå¢—åŠ ã™ã‚‹ã¨ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã®é€šä¿¡ï¼ˆã‚¯ã‚¨ãƒªï¼‰ã‚‚çˆ†ç™ºçš„ã«å¢—åŠ ã—ã¾ã™ã€‚
    ã“ã“ã§ã¯ã€å„AIãƒ¢ãƒ‡ãƒ«ã§ã®**ãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»é‡**ã¨**æ–™é‡‘**ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚
    
    **åˆ†æå¯¾è±¡AIï¼ˆãƒ•ãƒ©ãƒƒã‚°ã‚·ãƒƒãƒ—ãƒ¢ãƒ‡ãƒ«ï¼‰:**
    - ğŸŸ¢ **OpenAI GPT-4o**
    - ğŸ”µ **Anthropic Claude Sonnet 4**
    - ğŸ”´ **Google Gemini 1.5 Pro**
    """)
    
    if len(st.session_state.communication_log) > 0:
        df_log = pd.DataFrame(st.session_state.communication_log)
        
        # ãƒˆãƒ¼ã‚¯ãƒ³çµ±è¨ˆ
        total_tokens = df_log['tokens'].sum()
        total_cost_gpt4o = df_log['cost_gpt4o'].sum()
        total_cost_claude_sonnet = df_log['cost_claude_sonnet'].sum()
        total_cost_gemini_pro = df_log['cost_gemini_pro'].sum()
        
        # å††æ›ç®—
        total_cost_gpt4o_jpy = total_cost_gpt4o * USD_TO_JPY
        total_cost_claude_sonnet_jpy = total_cost_claude_sonnet * USD_TO_JPY
        total_cost_gemini_pro_jpy = total_cost_gemini_pro * USD_TO_JPY
        
        # ã‚µãƒãƒªãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        st.subheader("ğŸ“ˆ ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¨ã‚³ã‚¹ãƒˆ")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°", f"{total_tokens:,}")
            st.caption("å…¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®åˆè¨ˆ")
        
        with col2:
            st.metric("GPT-4o", f"Â¥{total_cost_gpt4o_jpy:.2f}")
            st.caption("OpenAI ãƒ•ãƒ©ãƒƒã‚°ã‚·ãƒƒãƒ—")
        
        with col3:
            st.metric("Claude Sonnet 4", f"Â¥{total_cost_claude_sonnet_jpy:.2f}")
            st.caption("Anthropic ãƒ•ãƒ©ãƒƒã‚°ã‚·ãƒƒãƒ—")
        
        with col4:
            st.metric("Gemini 1.5 Pro", f"Â¥{total_cost_gemini_pro_jpy:.2f}")
            st.caption("Google ãƒ•ãƒ©ãƒƒã‚°ã‚·ãƒƒãƒ—")
        
        st.markdown("---")
        
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        st.subheader("ğŸ¤– ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥ãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # é€ä¿¡å…ƒåˆ¥ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            tokens_by_sender = df_log.groupby('from')['tokens'].sum().sort_values(ascending=False)
            
            fig_tokens_sender = go.Figure()
            fig_tokens_sender.add_trace(go.Bar(
                x=tokens_by_sender.values,
                y=tokens_by_sender.index,
                orientation='h',
                marker=dict(
                    color=tokens_by_sender.values,
                    colorscale='Reds',
                    showscale=True,
                    colorbar=dict(title="ãƒˆãƒ¼ã‚¯ãƒ³æ•°")
                ),
                text=tokens_by_sender.values,
                texttemplate='%{text:,}',
                textposition='outside'
            ))
            
            fig_tokens_sender.update_layout(
                title="ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥é€ä¿¡ãƒˆãƒ¼ã‚¯ãƒ³æ•°",
                xaxis_title="ãƒˆãƒ¼ã‚¯ãƒ³æ•°",
                yaxis_title="ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ",
                height=400
            )
            
            st.plotly_chart(fig_tokens_sender, use_container_width=True)
        
        with col2:
            # ã‚³ã‚¹ãƒˆæ¯”è¼ƒï¼ˆ3ãƒ¢ãƒ‡ãƒ«ï¼‰- å††æ›ç®—
            cost_comparison = pd.DataFrame({
                'ãƒ¢ãƒ‡ãƒ«': ['GPT-4o', 'Claude Sonnet 4', 'Gemini 1.5 Pro'],
                'ã‚³ã‚¹ãƒˆ(JPY)': [
                    total_cost_gpt4o_jpy,
                    total_cost_claude_sonnet_jpy,
                    total_cost_gemini_pro_jpy
                ]
            })
            
            fig_cost_comparison = px.bar(
                cost_comparison,
                x='ãƒ¢ãƒ‡ãƒ«',
                y='ã‚³ã‚¹ãƒˆ(JPY)',
                title="AIãƒ¢ãƒ‡ãƒ«åˆ¥ã‚³ã‚¹ãƒˆæ¯”è¼ƒï¼ˆãƒ•ãƒ©ãƒƒã‚°ã‚·ãƒƒãƒ—ï¼‰",
                color='ã‚³ã‚¹ãƒˆ(JPY)',
                color_continuous_scale='RdYlGn_r',
                text='ã‚³ã‚¹ãƒˆ(JPY)'
            )
            
            fig_cost_comparison.update_traces(texttemplate='Â¥%{text:.2f}', textposition='outside')
            fig_cost_comparison.update_layout(height=400)
            st.plotly_chart(fig_cost_comparison, use_container_width=True)
        
        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®æ™‚ç³»åˆ—æ¨ç§»
        st.subheader("ğŸ“‰ ãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»ã®æ™‚ç³»åˆ—æ¨ç§»")
        
        df_log['cumulative_tokens'] = df_log['tokens'].cumsum()
        
        fig_timeline = go.Figure()
        
        fig_timeline.add_trace(go.Scatter(
            x=df_log.index,
            y=df_log['cumulative_tokens'],
            mode='lines',
            name='ç´¯ç©ãƒˆãƒ¼ã‚¯ãƒ³æ•°',
            line=dict(color='#667eea', width=3),
            fill='tozeroy',
            fillcolor='rgba(102, 126, 234, 0.2)'
        ))
        
        fig_timeline.update_layout(
            title="ç´¯ç©ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®æ¨ç§»ï¼ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã”ã¨ï¼‰",
            xaxis_title="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç•ªå·",
            yaxis_title="ç´¯ç©ãƒˆãƒ¼ã‚¯ãƒ³æ•°",
            height=400,
            hovermode='x unified'
        )
        
        st.plotly_chart(fig_timeline, use_container_width=True)
        
        # ğŸ’¥ ãƒ‡ãƒ¼ã‚¿é‡çˆ†å¢—ã®å¯è¦–åŒ–
        st.subheader("ğŸ’¥ IoTãƒ‡ãƒ¼ã‚¿é‡ vs ãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»ã®çˆ†å¢—")
        
        st.markdown("""
        **é‡è¦ãªç™ºè¦‹:**
        - ğŸ“Š 1,000ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ Ã— 20ã‚»ãƒ³ã‚µãƒ¼ = **20,000ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ**
        - ğŸ’¬ ã“ã‚Œã‚’ç•°å¸¸æ¤œçŸ¥ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«é€ä¿¡ã™ã‚‹ã¨ã€**æ•°ä¸‡ã€œæ•°åä¸‡ãƒˆãƒ¼ã‚¯ãƒ³**ã‚’æ¶ˆè²»
        - ğŸ’° å¤§è¦æ¨¡IoTã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€**æœˆé¡æ•°ç™¾ã€œæ•°åƒãƒ‰ãƒ«**ã®AIã‚³ã‚¹ãƒˆãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§
        """)
        
        col1, col2 = st.columns(2)
        
        with col1:
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—åˆ¥ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            tokens_by_type = df_log.groupby('type')['tokens'].sum().sort_values(ascending=False).head(10)
            
            fig_tokens_type = px.pie(
                values=tokens_by_type.values,
                names=tokens_by_type.index,
                title="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—åˆ¥ãƒˆãƒ¼ã‚¯ãƒ³åˆ†å¸ƒ",
                hole=0.4
            )
            
            fig_tokens_type.update_layout(height=400)
            st.plotly_chart(fig_tokens_type, use_container_width=True)
        
        with col2:
            # é«˜ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸TOP5
            top_token_messages = df_log.nlargest(5, 'tokens')[['from', 'to', 'type', 'tokens', 'size_kb']]
            
            st.markdown("### ğŸ”¥ é«˜ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ TOP5")
            
            for idx, row in top_token_messages.iterrows():
                st.markdown(f"""
                <div style="background-color: #fff3cd; padding: 1rem; border-radius: 5px; 
                            margin-bottom: 0.5rem; border-left: 4px solid #ffc107;">
                    <strong>{row['from']} â†’ {row['to']}</strong><br>
                    <small>{row['type']}</small><br>
                    ğŸ’¬ <strong>{row['tokens']:,} tokens</strong> | ğŸ“¦ {row['size_kb']:.2f} KB
                </div>
                """, unsafe_allow_html=True)
        
        # ã‚³ã‚¹ãƒˆè©³ç´°ãƒ†ãƒ¼ãƒ–ãƒ« - å††æ›ç®—
        st.subheader("ğŸ’µ è©³ç´°ã‚³ã‚¹ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«")
        
        cost_table = df_log[['from', 'to', 'type', 'tokens', 'size_kb', 
                             'cost_gpt4o', 'cost_claude_sonnet', 'cost_gemini_pro']].copy()
        
        # å††æ›ç®—
        cost_table['cost_gpt4o_jpy'] = cost_table['cost_gpt4o'] * USD_TO_JPY
        cost_table['cost_claude_sonnet_jpy'] = cost_table['cost_claude_sonnet'] * USD_TO_JPY
        cost_table['cost_gemini_pro_jpy'] = cost_table['cost_gemini_pro'] * USD_TO_JPY
        
        cost_table = cost_table[['from', 'to', 'type', 'tokens', 'size_kb', 
                                 'cost_gpt4o_jpy', 'cost_claude_sonnet_jpy', 'cost_gemini_pro_jpy']]
        cost_table.columns = ['é€ä¿¡å…ƒ', 'é€ä¿¡å…ˆ', 'ã‚¿ã‚¤ãƒ—', 'ãƒˆãƒ¼ã‚¯ãƒ³æ•°', 'ã‚µã‚¤ã‚º(KB)', 
                              'GPT-4o(JPY)', 'Claude Sonnet(JPY)', 'Gemini Pro(JPY)']
        
        st.dataframe(
            cost_table.tail(20),
            use_container_width=True,
            height=400
        )
        
        # æœˆé¡ã‚³ã‚¹ãƒˆäºˆæ¸¬ - å††æ›ç®—
        st.subheader("ğŸ“… æœˆé¡ã‚³ã‚¹ãƒˆäºˆæ¸¬")
        
        st.markdown("""
        **ä»®å®š:**
        - ã“ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ **1æ™‚é–“ã«1å›** å®Ÿè¡Œ
        - **24æ™‚é–“ Ã— 30æ—¥** = æœˆ720å›å®Ÿè¡Œ
        - **ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆ: $1 = Â¥155**
        """)
        
        monthly_multiplier = 720
        
        cost_projection = pd.DataFrame({
            'ãƒ¢ãƒ‡ãƒ«': ['GPT-4o', 'Claude Sonnet 4', 'Gemini 1.5 Pro'],
            '1å›ã‚ãŸã‚Š(JPY)': [
                total_cost_gpt4o_jpy,
                total_cost_claude_sonnet_jpy,
                total_cost_gemini_pro_jpy
            ],
            'æœˆé¡äºˆæ¸¬(JPY)': [
                total_cost_gpt4o_jpy * monthly_multiplier,
                total_cost_claude_sonnet_jpy * monthly_multiplier,
                total_cost_gemini_pro_jpy * monthly_multiplier
            ]
        })
        
        fig_monthly = go.Figure()
        
        fig_monthly.add_trace(go.Bar(
            x=cost_projection['ãƒ¢ãƒ‡ãƒ«'],
            y=cost_projection['æœˆé¡äºˆæ¸¬(JPY)'],
            text=cost_projection['æœˆé¡äºˆæ¸¬(JPY)'].apply(lambda x: f'Â¥{x:.0f}'),
            textposition='outside',
            marker=dict(
                color=['#10a37f', '#b575e3', '#4285f4'],  # OpenAI Green, Claude Purple, Google Blue
                line=dict(color='white', width=2)
            )
        ))
        
        fig_monthly.update_layout(
            title="æœˆé¡ã‚³ã‚¹ãƒˆäºˆæ¸¬ï¼ˆæœˆ720å›å®Ÿè¡Œæ™‚ï¼‰",
            xaxis_title="AIãƒ¢ãƒ‡ãƒ«",
            yaxis_title="æœˆé¡ã‚³ã‚¹ãƒˆ (JPY)",
            height=400,
            showlegend=False
        )
        
        st.plotly_chart(fig_monthly, use_container_width=True)
        
        # æœ€ã‚‚ã‚³ã‚¹ãƒˆåŠ¹ç‡ã®è‰¯ã„ãƒ¢ãƒ‡ãƒ« - å††æ›ç®—
        min_cost_model = cost_projection.loc[cost_projection['æœˆé¡äºˆæ¸¬(JPY)'].idxmin(), 'ãƒ¢ãƒ‡ãƒ«']
        min_cost_value = cost_projection['æœˆé¡äºˆæ¸¬(JPY)'].min()
        max_cost_model = cost_projection.loc[cost_projection['æœˆé¡äºˆæ¸¬(JPY)'].idxmax(), 'ãƒ¢ãƒ‡ãƒ«']
        max_cost_value = cost_projection['æœˆé¡äºˆæ¸¬(JPY)'].max()
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.success(f"""
            âœ… **æœ€ã‚‚ã‚³ã‚¹ãƒˆåŠ¹ç‡ãŒè‰¯ã„:** {min_cost_model}
            
            æœˆé¡äºˆæ¸¬: Â¥{min_cost_value:.0f}
            """)
        
        with col2:
            st.error(f"""
            âš ï¸ **æœ€ã‚‚ã‚³ã‚¹ãƒˆãŒé«˜ã„:** {max_cost_model}
            
            æœˆé¡äºˆæ¸¬: Â¥{max_cost_value:.0f}
            
            å·®é¡: Â¥{max_cost_value - min_cost_value:.0f} ({((max_cost_value / min_cost_value - 1) * 100):.1f}%å¢—)
            """)
        
        # é‡è¦ãªã‚¤ãƒ³ã‚µã‚¤ãƒˆ
        st.subheader("ğŸ’¡ é‡è¦ãªã‚¤ãƒ³ã‚µã‚¤ãƒˆ")
        
        st.markdown(f"""
        ### ğŸ”¥ ãƒ‡ãƒ¼ã‚¿é‡å¢—åŠ ã«ã‚ˆã‚‹ã‚³ã‚¹ãƒˆçˆ†å¢—
        
        ç¾åœ¨ã®è¨­å®š:
        - **ã‚»ãƒ³ã‚µãƒ¼æ•°:** 20å€‹
        - **ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ:** 1,000å€‹
        - **ç·ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ:** 20,000å€‹
        - **ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°:** {total_tokens:,}
        - **GPT-4oã§ã®1å›ã‚³ã‚¹ãƒˆ:** Â¥{total_cost_gpt4o_jpy:.2f}
        
        ### ğŸ“ˆ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚·ãƒŠãƒªã‚ª
        
        | ã‚·ãƒŠãƒªã‚ª | ã‚»ãƒ³ã‚µãƒ¼æ•° | ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ | äºˆæƒ³ãƒˆãƒ¼ã‚¯ãƒ³æ•° | GPT-4o æœˆé¡ã‚³ã‚¹ãƒˆ |
        |---------|----------|--------------|--------------|------------------|
        | ç¾åœ¨ | 20 | 1,000 | {total_tokens:,} | Â¥{total_cost_gpt4o_jpy * 720:.0f} |
        | ä¸­è¦æ¨¡ | 50 | 5,000 | {total_tokens * 12:,} | Â¥{total_cost_gpt4o_jpy * 720 * 12:.0f} |
        | å¤§è¦æ¨¡ | 100 | 10,000 | {total_tokens * 50:,} | Â¥{total_cost_gpt4o_jpy * 720 * 50:.0f} |
        | è¶…å¤§è¦æ¨¡ | 500 | 50,000 | {total_tokens * 1250:,} | Â¥{total_cost_gpt4o_jpy * 720 * 1250:.0f} |
        
        ### âš ï¸ çµè«–
        
        - IoTãƒ‡ãƒ¼ã‚¿é‡ãŒ**10å€**ã«ãªã‚‹ã¨ã€AIã‚³ã‚¹ãƒˆã‚‚**ç´„10å€**ã«å¢—åŠ 
        - å¤§è¦æ¨¡IoTã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€**ã‚³ã‚¹ãƒˆåŠ¹ç‡ã®è‰¯ã„AIãƒ¢ãƒ‡ãƒ«é¸æŠ**ãŒæ¥µã‚ã¦é‡è¦
        - **ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆ: $1 = Â¥155ã§è¨ˆç®—**
        - **ãƒ•ãƒ©ãƒƒã‚°ã‚·ãƒƒãƒ—ãƒ¢ãƒ‡ãƒ«é–“ã®æ¯”è¼ƒ:**
          - ğŸ¥‡ **Gemini 1.5 Pro**: æœ€ã‚‚ã‚³ã‚¹ãƒˆåŠ¹ç‡ãŒè‰¯ã„ï¼ˆå…¥åŠ›Â¥193.75/1M tokensï¼‰
          - ğŸ¥ˆ **GPT-4o**: ãƒãƒ©ãƒ³ã‚¹å‹ï¼ˆå…¥åŠ›Â¥387.50/1M tokensï¼‰
          - ğŸ¥‰ **Claude Sonnet 4**: æœ€é«˜å“è³ªã ãŒé«˜ã‚³ã‚¹ãƒˆï¼ˆå…¥åŠ›Â¥465.00/1M tokensï¼‰
        """)
        
    else:
        st.info("ğŸ‘† ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã€Œãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é–‹å§‹ã€â†’ã€Œæ¬¡ã¸ã€ã§ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„")

# ãƒ•ãƒƒã‚¿ãƒ¼
st.markdown("---")
col1, col2, col3 = st.columns(3)

with col1:
    st.markdown("**ã‚·ã‚¹ãƒ†ãƒ ãƒãƒ¼ã‚¸ãƒ§ãƒ³**")
    st.info("ğŸ“¦ v3.0.0 - AI Communication")

with col2:
    st.markdown("**æœ€çµ‚æ›´æ–°**")
    st.info(f"ğŸ• {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

with col3:
    st.markdown("**é€šä¿¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«**")
    st.success("âœ… Agent-to-Agent Protocol")
